{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "docusaurus": {
      "sidebar_position": 1,
      "title": "Getting Started",
      "description": "Build your first production-ready model in less than 5 minutes"
    },
    "colab": {
      "name": "Getting Started with Raptor",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raptor-ml/docs/blob/master/docs/docs/getting-started.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[↵ Back to the Docs](https://raptor.ml)\n",
        "\n",
        "<img src=\"https://raptor.ml/img/logo.svg\" height=\"200\" />\n",
        "\n",
        "# 🦖 LabSDK"
      ],
      "metadata": {
        "id": "BK8SRO9PQ5VS",
        "docusaurus_hide": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the LabSDK, data-scientists can build models(that can run on production) directly from the notebook.\n",
        "\n",
        "When you're done, you can \"export\" your work, like any other production asset. This way, you can **focus on your model**, while Raptor is taking care of the production concerns."
      ],
      "metadata": {
        "id": "olN5NKbsQlKB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🧐 Getting started\n",
        "In this quickstart tutorial, we'll build a model that predicts the probability of closing a deal.\n",
        "\n",
        "Our CRM allow us to track every email communication, and the history of previous deals for each customer. We'll use this data sources to predict whether the customer is ready for closure or not.\n",
        "\n",
        "To do that, we're going to build a few features from the data:\n",
        "1. `emails_10h` - the amount of email exchanges over the last 10 hours\n",
        "1. `question_marks_10h+avg` - the average amount of question marks in the subject over the last 10 hours\n",
        "1. `deals_10h+sum` - the sum of the deals of the last 10 hours\n",
        "1. `emails_deals` - the rate between the emails in the last 10 hours (`emails_10h`) and the avarage of the deals in the last 10 hours (`deals_10h[avg]`)\n",
        "1. `diff_with_previous_amount` - the delta between the last amount and the one before\n",
        "1. `last_amount` - our label"
      ],
      "metadata": {
        "id": "W8E2-YjxnEVt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ⚡️ Installing the SDK\n",
        "Yalla, let's go! First, we install the LabSDK and import it."
      ],
      "metadata": {
        "id": "ZwsP6pk7ibxZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install raptor-labsdk pyarrow -U --quiet\n",
        "from raptor import *\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from typing_extensions import TypedDict"
      ],
      "metadata": {
        "id": "a5eW89kWTdL5",
        "outputId": "f9cd22e0-8adf-46d6-e550-4e85fd5352b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m985.1/985.1 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.6/200.6 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.3/135.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.3/57.3 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.0/100.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.9/68.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.6/45.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ✍️ Writing our first features\n",
        "Our first feature is calculating how many emails an account got over the last 10 hours.\n",
        "\n",
        "To do that, we first define our data-sources, then we can start transforming our data."
      ],
      "metadata": {
        "id": "p88hB-owwk3-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@data_source(\n",
        "    training_data=pd.read_parquet('https://gist.github.com/AlmogBaku/8be77c2236836177b8e54fa8217411f2/raw/emails.parquet'),  # This is the data as looks in production\n",
        "    keys=['id', 'account_id'],\n",
        "    timestamp='event_at',\n",
        "    production_config=StreamingConfig(kind='kafka'), # This optional, and will create the production data-source configuration for DevOps\n",
        ")\n",
        "class Email(TypedDict('Email', {'from': str})):\n",
        "    event_at: datetime\n",
        "    account_id: str\n",
        "    subject: str\n",
        "    to: str"
      ],
      "metadata": {
        "id": "25sRcwoMwtR_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@feature(keys='account_id', data_source=Email)\n",
        "@aggregation(function=AggregationFunction.Count, over='10h', granularity='1h')\n",
        "def emails_10h(this_row: Email, ctx: Context) -> int:\n",
        "    \"\"\"email over 10 hours\"\"\"\n",
        "    return 1"
      ],
      "metadata": {
        "id": "aUqlADgYbcmr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@feature(keys='account_id', data_source=Email)\n",
        "@aggregation(function=AggregationFunction.Avg, over='10h', granularity='1h')\n",
        "def question_marks_10h(this_row: Email, ctx: Context) -> int:\n",
        "    \"\"\"question marks over 10 hours\"\"\"\n",
        "    return this_row['subject'].count('?')"
      ],
      "metadata": {
        "id": "xcHrGi2Nbmh4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "> ## 😎 *Cool tip*\n",
        ">\n",
        "> You can use the `@runtime` decorator to specify packages you want to install and use.\n",
        ">\n",
        "> [Learn more on the docs »](https://raptor.ml/)"
      ],
      "metadata": {
        "id": "1DiPXWqULKB7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create another feature that calculates various aggregations against the deal amount."
      ],
      "metadata": {
        "id": "PC6AOb9RxMau"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@data_source(\n",
        "    training_data=pd.read_csv('https://gist.githubusercontent.com/AlmogBaku/8be77c2236836177b8e54fa8217411f2/raw/deals.csv'),\n",
        "    keys=['id', 'account_id'],\n",
        "    timestamp='event_at',\n",
        ")\n",
        "class Deal(TypedDict):\n",
        "    id: int\n",
        "    event_at: datetime\n",
        "    account_id: str\n",
        "    amount: float"
      ],
      "metadata": {
        "id": "L0oPJvDgdKSY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@feature(keys='account_id', data_source=Deal)\n",
        "@aggregation(\n",
        "    function=[AggregationFunction.Sum, AggregationFunction.Avg, AggregationFunction.Max, AggregationFunction.Min],\n",
        "    over='10h',\n",
        "    granularity='1m'\n",
        ")\n",
        "def deals_10h(this_row: Deal, ctx: Context) -> float:\n",
        "    \"\"\"sum/avg/min/max of deal amount over 10 hours\"\"\"\n",
        "    return this_row['amount']"
      ],
      "metadata": {
        "id": "1xuZOPyayKHT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can create a *derived feature* that defines the rate between these two features.\n",
        "\n",
        "**💡Hint:** Notice that when querying a feature with aggregation, we need to specify the feature with the aggregation feature we want using the feature selector."
      ],
      "metadata": {
        "id": "Q1IADRr1yiIR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@feature(keys='account_id', sourceless_markers_df=Deal.raptor_spec.local_df)\n",
        "@freshness(max_age='-1', max_stale='-1')\n",
        "def emails_deals(_, ctx: Context) -> float:\n",
        "    \"\"\"emails/deal[avg] rate over 10 hours\"\"\"\n",
        "    e, _ = ctx.get_feature('emails_10h+count')\n",
        "    d, _ = ctx.get_feature('deals_10h+avg')\n",
        "    if e is None or d is None:\n",
        "        return None\n",
        "    return e / d"
      ],
      "metadata": {
        "id": "WOmMn2bxynMp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we'll create `last_amount` which will reserve one previous value. We'll use this feature as our label, and to calculte the delta between the previous amount."
      ],
      "metadata": {
        "id": "bxY7Vod0dht9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@feature(keys='account_id', data_source=Deal)\n",
        "@freshness(max_age='1h', max_stale='2h')\n",
        "@keep_previous(versions=1, over='1h')\n",
        "def last_amount(this_row: Deal, ctx: Context) -> float:\n",
        "    return this_row['amount']"
      ],
      "metadata": {
        "id": "7XEUpCv_d1iS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@feature(keys='account_id', sourceless_markers_df=Deal.raptor_spec.local_df)\n",
        "@freshness(max_age='1h', max_stale='2h')\n",
        "def diff_with_previous_amount(this_row: Deal, ctx: Context) -> float:\n",
        "    lv, ts = ctx.get_feature('last_amount@-1')\n",
        "    if lv is None:\n",
        "        return 0\n",
        "    return this_row['amount'] - lv"
      ],
      "metadata": {
        "id": "Dp1PzC9Xd27X"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🧠 Training our model\n",
        "After we defined our features, and wrote our feature engineering code, we can start and train our model."
      ],
      "metadata": {
        "id": "Lpfe-PHWqAVY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@model(\n",
        "    keys=['account_id'],\n",
        "    input_features=[\n",
        "        'emails_10h+count', 'deals_10h+sum', emails_deals, diff_with_previous_amount, 'question_marks_10h+avg',\n",
        "    ],\n",
        "    input_labels=[last_amount],\n",
        "    model_framework='sklearn',\n",
        "    model_server='sagemaker-ack',\n",
        ")\n",
        "@freshness(max_age='1h', max_stale='100h')\n",
        "def deal_prediction(ctx: TrainingContext) -> float:\n",
        "    from xgboost import XGBClassifier\n",
        "    from sklearn.model_selection import train_test_split\n",
        "\n",
        "    df = ctx.features_and_labels()\n",
        "    X = df[ctx.input_features]\n",
        "    y = df[ctx.input_labels]\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "    # Transform y_train to a 1D array\n",
        "    y_train = y_train.values.ravel()\n",
        "\n",
        "    # Initialize an XGBoost model\n",
        "    xgb_model = XGBClassifier()\n",
        "\n",
        "    # Fit the model to the training data\n",
        "    from sklearn.preprocessing import LabelEncoder\n",
        "    le = LabelEncoder()\n",
        "\n",
        "    # Initialize the LabelEncoder\n",
        "    le = LabelEncoder()\n",
        "    y_train_encoded = le.fit_transform(y_train)\n",
        "\n",
        "    # Fit the model with the encoded labels\n",
        "    xgb_model.fit(X_train, y_train_encoded)\n",
        "\n",
        "    return xgb_model\n",
        "\n",
        "deal_prediction.export()"
      ],
      "metadata": {
        "id": "tNeaKZGMErJ4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ☁️ Deployment\n",
        "Your'e officially done! 🎉\n",
        "\n",
        "To deploy your model, instruct your DevOps team to deploy it using the existing CI/CD using the generated `Makefile` in the `out` dir (or manually using `kubectl`)."
      ],
      "metadata": {
        "id": "GO99n3tCpC2y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!make -C out/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ESVUlESq3QC",
        "outputId": "667c80f4-391f-4f87-8784-841136e02062"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;97m                    █▀\n",
            "\u001b[38;5;97m                  █▀  █▀\n",
            "\u001b[38;5;97m▄               ██▀ ▄█▀  ▄\n",
            "\u001b[38;5;97m █▄▄           ██▀ ▄█▀ ▄█▀\n",
            "\u001b[38;5;97m   ▀▀██████████▄ █ ▄▄█▀\n",
            "\u001b[38;5;97m    █▄        ███ █▀                                 ▄\n",
            "\u001b[38;5;97m    ███      ███ ▀                                ▂▄██\n",
            "\u001b[38;5;97m    ███     ████     ▄██▀▀▀████   ██ ▄█▀▀▀▀█▄    ▀███▀▀▀▀   ▄███▀▀▀███▄   █ ▄█▀▀▀▀\n",
            "\u001b[38;5;97m    ███████████▎   ▄██▀     ▀██   ██▀      ▀██▄   ███      ███       ██▌  ██▀\n",
            "\u001b[38;5;97m    ███     ▀███   ██▌      ▄██   ██▌       ██▀   ██       ██        ██   ██\n",
            "\u001b[38;5;97m    ███      ▐██▄  ▀██▄   ▄▀███   ███     ▄██▀    ███       ███     ██    █▌\n",
            "\u001b[38;5;97m    ███       ███▄   ▀▀██▀▀  ██   ███▀▀▀▀▀▀       ▀███▀       ▀████▀      █\n",
            "\u001b[38;5;97m                                  ██▌\n",
            "\u001b[38;5;97m                                  █▀\n",
            "\u001b[0m\n",
            "\n",
            "Usage:\n",
            "  make \u001b[36m<target>\u001b[0m\n",
            "\n",
            "\u001b[1mGeneral\u001b[0m\n",
            "  \u001b[36mhelp           \u001b[0m  Display this help.\n",
            "\n",
            "\u001b[1mHelpers\u001b[0m\n",
            "  \u001b[36mshow-envs      \u001b[0m  Show all environment variables that are available for configuring the generated YAML manifests\n",
            "\n",
            "\u001b[1mAll\u001b[0m\n",
            "  \u001b[36mall            \u001b[0m  Build docker images for all models, push them to the docker repository and deploy all data-sources, features and models to Kubernetes\n",
            "  \u001b[36mdeploy         \u001b[0m  Deploy all data-sources, features and models to Kubernetes\n",
            "  \u001b[36mall-ecr        \u001b[0m  Build docker images for all models, create ECR repos if not exists, push the images to the docker repository and deploy all data-sources, features and models to Kubernetes\n",
            "  \u001b[36mdeploy-ecr     \u001b[0m  Deploy all data-sources, features and models to Kubernetes\n",
            "\n",
            "\u001b[1mData Sources\u001b[0m\n",
            "  \u001b[36mdeploy-dsrcs   \u001b[0m  Deploy all data-sources to Kubernetes\n",
            "  \u001b[36mdeploy-dsrc-default-email\u001b[0m  Deploy default.email to Kubernetes\n",
            "  \u001b[36mdeploy-dsrc-default-deal\u001b[0m  Deploy default.deal to Kubernetes\n",
            "\n",
            "\u001b[1mFeatures\u001b[0m\n",
            "  \u001b[36mdeploy-features\u001b[0m  Deploy all features to Kubernetes\n",
            "  \u001b[36mdeploy-feat-default-emails_10h\u001b[0m  Deploy default.emails_10h to Kubernetes\n",
            "  \u001b[36mdeploy-feat-default-deals_10h\u001b[0m  Deploy default.deals_10h to Kubernetes\n",
            "  \u001b[36mdeploy-feat-default-emails_deals\u001b[0m  Deploy default.emails_deals to Kubernetes\n",
            "  \u001b[36mdeploy-feat-default-diff_with_previous_amount\u001b[0m  Deploy default.diff_with_previous_amount to Kubernetes\n",
            "  \u001b[36mdeploy-feat-default-question_marks_10h\u001b[0m  Deploy default.question_marks_10h to Kubernetes\n",
            "\n",
            "\u001b[1mModels (All)\u001b[0m\n",
            "  \u001b[36mdeploy-models  \u001b[0m  Deploy all models to Kubernetes\n",
            "  \u001b[36mdocker-build-models\u001b[0m  Build docker images for all models\n",
            "  \u001b[36mdocker-push-models\u001b[0m  Push docker images for all models\n",
            "  \u001b[36mcreate-model-ecr-repos\u001b[0m  Create ECR repositories for all models if they don't exist\n",
            "  \u001b[36mdocker-ecr-push-models\u001b[0m  Push docker images for all models to ECR\n",
            "  \u001b[36mdeploy-ecr-models\u001b[0m  Deploy all models to Kubernetes\n",
            "\n",
            "\u001b[1mModels.PHONY: create-model-ecr-repo-default-deal_prediction\u001b[0m\n",
            "  \u001b[36mcreate-model-ecr-repo-default-deal_prediction\u001b[0m  Create ECR repository for default.deal_prediction if it doesn't exist\n",
            "  \u001b[36mdocker-ecr-push-model-default-deal_prediction\u001b[0m  Push docker image for default.deal_prediction to ECR\n",
            "  \u001b[36mdeploy-ecr-model-default-deal_prediction\u001b[0m  Deploy default.deal_prediction to ECR.PHONY: docker-build-model-default-deal_prediction\n",
            "  \u001b[36mdocker-build-model-default-deal_prediction\u001b[0m  Build docker image for default.deal_prediction\n",
            "  \u001b[36mdocker-push-model-default-deal_prediction\u001b[0m  Push default.deal_prediction docker image\n",
            "  \u001b[36mdeploy-model-default-deal_prediction\u001b[0m  Deploy default.deal_prediction to Kubernetes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🪄 Ta-dam!\n",
        "**From now on**, you'll have features and models running in production and record the values for historical purposes (so you'll be able to retrain against the production data).\n",
        "\n",
        "[**🔗 Learn more about what else you can do with Raptor at the official docs**](https://raptor.ml)\n"
      ],
      "metadata": {
        "id": "i8UXJLnwo9jC"
      }
    }
  ]
}
