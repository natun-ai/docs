"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[6806],{4907:(e,t,n)=>{n.d(t,{Zo:()=>p,kt:()=>m});var i=n(9953);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);t&&(i=i.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,i)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,i,a=function(e,t){if(null==e)return{};var n,i,a={},r=Object.keys(e);for(i=0;i<r.length;i++)n=r[i],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(i=0;i<r.length;i++)n=r[i],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var s=i.createContext({}),u=function(e){var t=i.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},p=function(e){var t=u(e.components);return i.createElement(s.Provider,{value:t},e.children)},d="mdxType",h={inlineCode:"code",wrapper:function(e){var t=e.children;return i.createElement(i.Fragment,{},t)}},c=i.forwardRef((function(e,t){var n=e.components,a=e.mdxType,r=e.originalType,s=e.parentName,p=l(e,["components","mdxType","originalType","parentName"]),d=u(n),c=a,m=d["".concat(s,".").concat(c)]||d[c]||h[c]||r;return n?i.createElement(m,o(o({ref:t},p),{},{components:n})):i.createElement(m,o({ref:t},p))}));function m(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var r=n.length,o=new Array(r);o[0]=c;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l[d]="string"==typeof e?e:a,o[1]=l;for(var u=2;u<r;u++)o[u]=n[u];return i.createElement.apply(null,o)}return i.createElement.apply(null,n)}c.displayName="MDXCreateElement"},8946:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>o,default:()=>d,frontMatter:()=>r,metadata:()=>l,toc:()=>u});var i=n(1943),a=(n(9953),n(4907));const r={title:"Raptor Spec",sidebar_label:"Architectural Design",sidebar_position:99,id:"spec"},o="High level design",l={unversionedId:"reference/spec",id:"reference/spec",title:"Raptor Spec",description:"Feature Definitions are an abstraction that contains metadata about the feature that should lead to create a Feature Value.",source:"@site/docs/reference/spec.md",sourceDirName:"reference",slug:"/reference/spec",permalink:"/docs/reference/spec",draft:!1,editUrl:"https://github.com/raptor-ml/docs/tree/master/docs/reference/spec.md",tags:[],version:"current",sidebarPosition:99,frontMatter:{title:"Raptor Spec",sidebar_label:"Architectural Design",sidebar_position:99,id:"spec"},sidebar:"reference",previous:{title:"time",permalink:"/docs/reference/pyexp/raptor-built-ins/time"}},s={},u=[{value:"Feature Value",id:"feature-value",level:2},{value:"Kubernetes Controller",id:"kubernetes-controller",level:2},{value:"Core&#39;s engine",id:"cores-engine",level:2},{value:"Core&#39;s operator",id:"cores-operator",level:2},{value:"Low-level API",id:"low-level-api",level:2},{value:"Online-aggregations",id:"online-aggregations",level:2},{value:"Snapshotting",id:"snapshotting",level:2},{value:"Regular features snapshotting",id:"regular-features-snapshotting",level:3},{value:"Windowed features snapshotting",id:"windowed-features-snapshotting",level:3},{value:"Synchronization process",id:"synchronization-process",level:2},{value:"Collect Notifications",id:"collect-notifications",level:3},{value:"Write jobs:",id:"write-jobs",level:3}],p={toc:u};function d(e){let{components:t,...r}=e;return(0,a.kt)("wrapper",(0,i.Z)({},p,r,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"high-level-design"},"High level design"),(0,a.kt)("mermaid",{value:'flowchart\n    subgraph Core\n        direction TB\n        engine --\x3e pipeline\n        ctrl[Kubernetes Controller] & acc["Accessor<br><i>(REST & gRPC APIs)</i>"] <--\x3e engine\n        subgraph Engine\n            engine\n            pipeline --\x3e middlewares --\x3e state\n            \n            subgraph state\n            direction TB\n                p[state provider]\n                win[window fns]\n                notify\n            end\n\n            subgraph middlewares\n                direction TB\n                PyExp\n                REST\n                GeoIP\n                gRPC\n                encdoing\n                validations\n                ...\n            end\n        end\n    end\n\n    redis[("distributed state <br><i>(redis)</i>")] <-----\x3e state & sub\n    ft[Feature CRD] --\x3e ctrl\n    dconn[DataConnctor CRD] --\x3e ctrl\n    fs[FeatureSet CRD] --\x3e ctrl\n\n    subgraph runners\n        direction LR\n        subgraph webhook\n            wh[runner] <--\x3e r[runtime] --\x3e acc\n        end\n        subgraph streaming\n            st[runner] <--\x3e r2[runtime] --\x3e acc\n        end\n    end\n    subgraph historian\n        direction LR\n        sub[notifications subscriber] --\x3e snapshotter & write\n        snapshotter --\x3e write\n    end\n\n    write --\x3e dw[(datalake: s3 / snowflake)]'}),(0,a.kt)("h1",{id:"feature-definitions"},"Feature Definitions"),(0,a.kt)("p",null,"Feature Definitions are an abstraction that contains metadata about the feature that should lead to create a Feature Value."),(0,a.kt)("p",null,"Feature definitions contains:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"FQN (Fully Qualified Name): a unique name which is the composition of ",(0,a.kt)("inlineCode",{parentName:"li"},"{name}.{namespace}[aggr_fn?]")),(0,a.kt)("li",{parentName:"ul"},"Primitive Type:"),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"Int")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"String")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"Float")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"Timestamp")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"[]Int")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"[]String")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"[]Float")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"[]Timestamp")),(0,a.kt)("li",{parentName:"ul"},"Freshness duration - The duration which within the feature value is considered as fresh, and shouldn't be recalculated"),(0,a.kt)("li",{parentName:"ul"},"Staleness duration - The duration which after it the feature is considered as stale and ",(0,a.kt)("em",{parentName:"li"},"is invalid")," to be used.\nThe state should evict/expire that value after that, and only story it for historical purposes."),(0,a.kt)("li",{parentName:"ul"},"Timeout duration - The maximum time the server should respond with a feature value. In this time it tries to get\nthe most fresh value it can provide (considering the above constrains)")),(0,a.kt)("h2",{id:"feature-value"},"Feature Value"),(0,a.kt)("p",null,"The Feature Value is the computed value of a feature's ingestion."),(0,a.kt)("p",null,"Properties:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"fqn")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"entity_id")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"value")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"timestamp"))),(0,a.kt)("h1",{id:"core"},"Core"),(0,a.kt)("p",null,'The Core is the main component of Raptor. It acts as a "compiler" for the Raptor Feature Definitions, and responsible\nfor the "production" implementation for these definitions.'),(0,a.kt)("p",null,"The Core is responsible to:"),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},"Read the Feature Definitions"),(0,a.kt)("li",{parentName:"ol"},'Create an appropriate "builder" for each Feature Definition using the internal ',"[read/write pipelines][#Pipleines]",",\nand ","[runners][#Data-ingestion]"),(0,a.kt)("li",{parentName:"ol"},"Maintain the orchestration and monitoring of the system")),(0,a.kt)("p",null,"It has a few key components:"),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},"[Engine][#Cores-engine]"," - responsible for the feature pipeline implementation"),(0,a.kt)("li",{parentName:"ol"},"[Operator][#Cores-operator]"," - responsible for the orchestration of the system(including spawning different services)"),(0,a.kt)("li",{parentName:"ol"},"State- responsible for the state of the system, including the state of the features and the\nstate of the system itself"),(0,a.kt)("li",{parentName:"ol"},"Accesssor - responsible for the access to the state of the system via the API"),(0,a.kt)("li",{parentName:"ol"},"Plugin system - responsible for the loading of the plugins")),(0,a.kt)("h2",{id:"kubernetes-controller"},"Kubernetes Controller"),(0,a.kt)("p",null,"The Kubernetes controller enables the user to use Raptor using the Kubernetes world. It is implemented as a part of the\nCore."),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"The controller supports validation on time of manifest create"),(0,a.kt)("li",{parentName:"ul"},'It "read and implement" the CRDs',(0,a.kt)("ul",{parentName:"li"},(0,a.kt)("li",{parentName:"ul"},"Feature - defines a feature metadata & business logic"),(0,a.kt)("li",{parentName:"ul"},"DataConnector - defines a connector configuration (i.e. how to connect to Kafka, what's the creds, what's the\ntopic,\nschema, allocated resources, etc.)"),(0,a.kt)("li",{parentName:"ul"},"FeatureSet - defines a set of features."))),(0,a.kt)("li",{parentName:"ul"},"Update operations for Features are blocked by default and will be enabled ",(0,a.kt)("em",{parentName:"li"},"only by enabling a special flag"),".")),(0,a.kt)("h2",{id:"cores-engine"},"Core's engine"),(0,a.kt)("p",null,"The Core's engine is responsible for accessing and processing the feature values, as well as storing them in the State,\nand executing pipelines."),(0,a.kt)("h2",{id:"cores-operator"},"Core's operator"),(0,a.kt)("p",null,"The Core's operator is responsible for the orchestration of the system. It is responsible for the following:"),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},"Spawning the different services such as Runners"),(0,a.kt)("li",{parentName:"ol"},"Modifying the CR's status"),(0,a.kt)("li",{parentName:"ol"},"Implementing CR's webhooks")),(0,a.kt)("h2",{id:"low-level-api"},"Low-level API"),(0,a.kt)("p",null,"The low-level API (aka Engine API) support low-level operations over feature values:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-go"},"// Engine is the main engine of the Core\n// It is responsible for the low-level operation for the features against the feature store\ntype Engine interface {\n    // Metadata returns the metadata of the feature\n    Metadata(ctx context.Context, FQN string) (Metadata, error)\n    // Get returns the SimpleValue of the feature.\n    // If the feature is not available, it returns nil.\n    // If the feature is windowed, the returned SimpleValue is a map from window function to SimpleValue.\n    Get(ctx context.Context, FQN string, entityID string) (Value, Metadata, error)\n    // Set sets the SimpleValue of the feature.\n    // If the feature's primitive is a List, it replaces the entire list.\n    // If the feature is windowed, it is aliased to WindowAdd instead of Set.\n    Set(ctx context.Context, FQN string, entityID string, val any, ts time.Time) error\n    // Append appends the SimpleValue to the feature.\n    // If the feature's primitive is NOT a List it will throw an error.\n    Append(ctx context.Context, FQN string, entityID string, val any, ts time.Time) error\n    // Incr increments the SimpleValue of the feature.\n    // If the feature's primitive is NOT a Scalar it will throw an error.\n    // It returns the updated value in the state, and an error if occurred.\n    Incr(ctx context.Context, FQN string, entityID string, by any, ts time.Time) error\n    // Update is the common function to update a feature SimpleValue.\n    // Under the hood, it utilizes lower-level functions depending on the type of the feature.\n    //  - Set for Scalars\n    //  - Append for Lists\n    //  - WindowAdd for Windows\n    Update(ctx context.Context, FQN string, entityID string, val any, ts time.Time) error\n}\n")),(0,a.kt)("p",null,"It is exposed via the Accessor as gRPC and Rest."),(0,a.kt)("h2",{id:"online-aggregations"},"Online-aggregations"),(0,a.kt)("p",null,"The engine support online aggregation (using the bucket algorithm)"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"SUM"),(0,a.kt)("li",{parentName:"ul"},"MAX"),(0,a.kt)("li",{parentName:"ul"},"MIN"),(0,a.kt)("li",{parentName:"ul"},"AVG"),(0,a.kt)("li",{parentName:"ul"},"COUNT")),(0,a.kt)("h1",{id:"pipelines"},"Pipelines"),(0,a.kt)("p",null,'Getting and Setting values to the State is done using the "pipeline". The pipeline is composed by a chain of middleware\nfunctions that wraps the access to/from the State by the priority(the lowest priority, the earlier it\'s executed) that\neach middleware is defined with:'),(0,a.kt)("mermaid",{value:'graph LR\n  subgraph Get Pipeline\n    PreG["Pre Get #1"] -- 1...n --\x3e PreG99["Pre Get #99"] --\x3e Get[Get from the State] --\x3e PostG["Post Get #1"] -- 1...n --\x3e PostG99["Post Get #99"]\n  end\n  subgraph Set Pipeline\n    PreS["Pre Set #1"] -- 1...n --\x3e PreS99["Pre Set #99"] --\x3e Set[Set to the State] --\x3e PostS["Post Set #1"] -- 1...n --\x3e PostS99["Post Set #99"]\n  end'}),(0,a.kt)("p",null,"The middlewares allow chaining a set of functions when getting or setting a value, in order to mutate the value (or to\nprevent the operation). This is useful for implementing a variety of features, including:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Validations - validations are PreSet middlewares"),(0,a.kt)("li",{parentName:"ul"},"DataConnectors that retrieve the data on the time-of-request (i.e. REST API)"),(0,a.kt)("li",{parentName:"ul"},"Transformations"),(0,a.kt)("li",{parentName:"ul"},'Encoders - i.e. attach "hashing" for the value at PostGet')),(0,a.kt)("h1",{id:"builders"},"Builders"),(0,a.kt)("p",null,"Builders are the composition of all the elements required to create a feature value:"),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},"Builder Kind - specifies the type of the builder composer. This is the unit that composing the pipeline and the\nimplementation details of the builder (and sometimes responsible for the connector implementation)."),(0,a.kt)("li",{parentName:"ol"},"Aggregations definition (if any)"),(0,a.kt)("li",{parentName:"ol"},"PyExp code")),(0,a.kt)("p",null,"To build features, the builder required to pull configurations from two places:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"DataConnector CRD - defining the connection configuration"),(0,a.kt)("li",{parentName:"ul"},"Feature CRD - defining the business logic of a feature creation\n",(0,a.kt)("img",{alt:"Shape1",src:n(5851).Z,width:"1152",height:"864"}))),(0,a.kt)("admonition",{type:"tip"},(0,a.kt)("p",{parentName:"admonition"},"Builders are an amorphic concept that unite together a set of instructions how to build the feature value.\nThere is no one unit that implements it.")),(0,a.kt)("h1",{id:"data-ingestion"},"Data ingestion"),(0,a.kt)("p",null,"Data ingestion can be implemented externally or internally"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},'Externally - as a standalone service (aka "Runner" deployment)\nThis can be implemented by writing a custom microservice (i.e. for webhook or streaming connectors)')),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},'Internally - in-process of the "Core", utilizing the GET/SET pipeline.\nThis can be implemented by writing a "plugin" to that adds a middleware for this feature'))),(0,a.kt)("p",null,'Writing data as "feature value" is done using the "low-level API" (whether it\'s internally via the library or externally\nby the runtime sidecar)'),(0,a.kt)("h1",{id:"historian"},"Historian"),(0,a.kt)("p",null,"The historian is responsible to keep records of the (current) State (of the world), to a storage.\nIt does that by scheduling a periodic snapshotting of the State to the storage."),(0,a.kt)("h2",{id:"snapshotting"},"Snapshotting"),(0,a.kt)("p",null,"Snapshotting is the process that is responsible for copying data from the real-time/online state to the historical\nstorage."),(0,a.kt)("p",null,"This process is composed of 3 different sub-processes:"),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("strong",{parentName:"li"},"Regular features snapshotting")),(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("strong",{parentName:"li"},"Scheduled Windowed-features snapshotting")),(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("strong",{parentName:"li"},"Storing process")," - Synchronizes write calls")),(0,a.kt)("h3",{id:"regular-features-snapshotting"},"Regular features snapshotting"),(0,a.kt)("p",null,"We are keeping every ",(0,a.kt)("strong",{parentName:"p"},"change")," to the feature. To do that we just need to hook in just after the write in the pipeline."),(0,a.kt)("p",null,"Every feature's writing request in the pipeline triggers writing to a distributed queue:"),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},"Pipeline: write"),(0,a.kt)("li",{parentName:"ol"},"\u2192Pipeline: Go routing to ",(0,a.kt)("inlineCode",{parentName:"li"},"Historian.AddWriteNotification(fqn, entityId,value)")),(0,a.kt)("li",{parentName:"ol"},"\u2192Historian library: write a ",(0,a.kt)("em",{parentName:"li"},"notification message")," to a Redis stream")),(0,a.kt)("h3",{id:"windowed-features-snapshotting"},"Windowed features snapshotting"),(0,a.kt)("p",null,"Due to the different behavior and volatility of windowed features, a different implementation is required:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("em",{parentName:"li"},"Windowed features")," are prune to MANY writes (due to the fact they are used to storing aggregations)"),(0,a.kt)("li",{parentName:"ul"},"Copying every change is expensive, inefficient and duplicating the raw data under the hood."),(0,a.kt)("li",{parentName:"ul"},"We are triggering snapshotting of windowed features by:",(0,a.kt)("ul",{parentName:"li"},(0,a.kt)("li",{parentName:"ul"},"A periodic snapshotting (every 5 minutes)"),(0,a.kt)("li",{parentName:"ul"},'Trigger an "update notification" when a windowed feature is written to')))),(0,a.kt)("p",null,"To support the above, we are required to keep buckets in the state a little longer to make sure we're collecting them."),(0,a.kt)("h2",{id:"synchronization-process"},"Synchronization process"),(0,a.kt)("p",null,"The Synchronization process is running only on a leader instance.\nIt utilizes an internal job queue combines duplicated ",(0,a.kt)("em",{parentName:"p"},"writes")," and ",(0,a.kt)("em",{parentName:"p"},"collection")," notifications, and handles them."),(0,a.kt)("h3",{id:"collect-notifications"},"Collect Notifications"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"collect the values"),(0,a.kt)("li",{parentName:"ul"},"Add a ",(0,a.kt)("em",{parentName:"li"},"writing job")," of the results")),(0,a.kt)("h3",{id:"write-jobs"},"Write jobs:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"The Writing process is running only on a leader instance."),(0,a.kt)("li",{parentName:"ul"},"Writing the historical records to Historical Provider (i.e. Parquet S3, Snowflake, BigQuery, etc.)")),(0,a.kt)("h1",{id:"glossary"},"Glossary"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"CRD - Custom Resource Definition"),(0,a.kt)("li",{parentName:"ul"},"Feature - A data input for the model which describes a trait of our data"),(0,a.kt)("li",{parentName:"ul"},"Feature Definition/Manifest - A feature's business logic declarative code. This is used by data scientists(via the\nLabSDK) to describe how the Core should generate feature values from ",(0,a.kt)("em",{parentName:"li"},"raw data")," and how the platform should store it."),(0,a.kt)("li",{parentName:"ul"},"FeatureSet - A set of features that are used to serve a specific model."),(0,a.kt)("li",{parentName:"ul"},"DataConnector - A ",(0,a.kt)("em",{parentName:"li"},"conceptual")," unit that retrieves the data from the production system. Sometimes it's an actual unit(\ni.e. Kafka Runner), and sometimes it's just configuring a program."),(0,a.kt)("li",{parentName:"ul"},"DataConnector Manifest - The configuration of the DataConnector"),(0,a.kt)("li",{parentName:"ul"},"Core - The main platform's program. This is the unit that is responsible to glue it all together"),(0,a.kt)("li",{parentName:"ul"},"Read/Write pipeline - The pipeline of fetching/setting data from/to the storage. At its middle, we have the actual\noperation of the storage."),(0,a.kt)("li",{parentName:"ul"},"Middlewares - The steps that wrap the read/write of the store"),(0,a.kt)("li",{parentName:"ul"},"Runner - A unit that is running outside the Core and responsible"),(0,a.kt)("li",{parentName:"ul"},"Historical Provider - the historical storage implementation. We're taking snapshots of the current state to it."),(0,a.kt)("li",{parentName:"ul"},"State - The state is the unit the stores the feature values to be served to models. It describes the current state of\nthe world.")))}d.isMDXComponent=!0},5851:(e,t,n)=>{n.d(t,{Z:()=>i});const i=n.p+"assets/images/spec.builders-bd7aa9984b41f9f5ff180834deb1488b.png"}}]);